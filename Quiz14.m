1번.
"Classification 알고리즘의 종류가 아닌 것을 고르시오."

1) Linear Regreession (X)
- 연속적인 값을 예측하는 데 사용되는 회귀 알고리즘임.
- Classification에 직접 사용되지 앙ㄶ으며, 주로 출력값이 연속적일 때 사용됨

2) Perceptron
- 이진 분류를 수행하는 선형 분류 알고리즘임.
단층 신경망으로 구성되어 있으며, 데이터가 선형적으로 구분 가능한 경우 사용됨

3) SVM
- SVM은 데이터의 결정 경계를 최적화하여 분류를 수행하는 classification 알고리즘임.
선형 및 비선형 분류 모두 가능하며 kernel trick을 활용할 수 있음.

4) Logistic Regression
- 분류 문제를 해결하기 위한 알고리즘임. 연속적인 선형 회귀를 기반으로 하며,
sigmoid 함수를 사용하여 확률 값을 계싼하고, 이 확률로 클래스 예측을 수행

=============================================================================================

2번.
"logistic regression은 모든 데이터 포인트까지의 거리의 정보를 사용한다."라는 진술이 맞는지 확인

틀림.
- Logistic regression은 확률 값을 계산하는 알고리즘으로, 모든 데이터 포인트까지의 거리 정보를
사용하지 않음.
(거리 정보를 기반하는 알고리즘은 svm과 같은 마진 기반 분류기임)

==================================================================================================

3번.
logistic regression은 모든 데이터 포인트까지의 거리를 고려하는 알고리즘으로, outlier의 영향을 많이 받습니다. 이를 보완하기 위해 사용하는 방법을 묻고 있습니다.

Sigmoid Function

====================================================================================================

4번.
"sigmoid function σ(z)에 대한 설명으로 옳지 않은 것을 고르시오."

1) monotonic이다.
- 시그모이드함수는 항상 증가하는 함수로, 입력이 증가하면 출력도 증가 (단조함수)
monotonic(단조)함수임.

2) 시그모이드 함수는 대칭적인 특성을 지님.

3) 미분 불가능하다.
-> 시그모이드 함수는 미분 가능한 함수임. 

4) 연속 함수이다.
-> 시그모이드 함수는 z의 모든 값에 대해 연속적이며, 매끄럽게 정의된 함수임

=================================================================================================

5번.
"의사결정나무 모델이 데이터를 분기하는 기준이 되는 척도는 무엇임"?

1) 회귀분석( X)
- 회귀분석은 연속적인 값을 예측하기 위한 분석 방법으로, 의사결정나무에서 데이터를
분기하는 데 사용되지 않음

2) 엔트로피 OR Quality of test (O)
- 의사결정나무는 데이터를 분기하기 위해 엔트로피나 지니계수와 같은 척도를 사용

3) 유사도 (X)
- 유사도는 데이터를 비교하거나 군집화할 때 주로 사용

4) 중심경향성 (X)
- 중심경향성은 데이터의 평균, 중앙값 등과 관련된 통계적 척도로, 의사결정나무 분기 기준과 무관

===================================================================================================

6번.
"의사결정나무의 장점 중 하나가 아닌 것은 무엇인가요?"

1. 해석이 직관적이다 (장점)
- 트리 구조로 이루어져 있어 시각적 이해 쉬움.

2. 비선형 관계를 잘 학습할 수 있다. (장점)
- 선형 모델과 달리 데이터를 분기하여 비선형적인 관계도 학습할 수 있음

3. 과적합을 항상 방지한다. (x)
- 의사결정나무는 과적합에 취약할 수 있음. 과적합을 방지하려면
트리의 깊이를 제한하거나, 가지치기와 같은 추가적인 조치 필요

4. 명확한 시각화가 가능함 (장점)

=======================================================================================================

7번.
"의사결정나무를 앙상블 기법에 적용하여 성능을 향상시킨 모델은 무엇인가요"

1. 랜덤 포레스트(정답)

2. k-최근접 이웃(오답)

3. 선형 회귀(오답)

4. 서포트 벡터 머신(오답)

#=================================================================================================

8번. 매트랩에서 의사결정나무는 fitctree 함수를 사용하여 생성함.

ex.
clf = ___(x, y, 'SplitCriterion', 'gdi', 'MaxNumSplits', 3);
SplitCriterion: 분할 기준을 설정하며, 'gdi'(지니 불순도)가 사용됨.
MaxNumSplits: 최대 분할 수를 3으로 제한.