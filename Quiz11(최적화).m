# 1ë²ˆ
1. Under-determined linear systemì—ì„œëŠ” ë¯¸ì§€ìˆ˜ì˜ ìˆ˜ê°€ constraintë³´ë‹¤ ë§ì•„ í•´ê°€ ë§ì´ ì¡´ì¬í•œë‹¤. (O)
2. ğ´ê°€ mÃ—n í–‰ë ¬ì¼ ë•Œ m>nì´ë©´ under-determined linear systemì´ë‹¤. (X)
3. Over-determined linear systemì€ fatí•œ ğ´í–‰ë ¬ì„ ê°€ì§„ë‹¤. (X)
4. Aê°€ ì •ì‚¬ê°í–‰ë ¬ì´ë©´ í•­ìƒ well-determined linear systemì´ë‹¤. (X)

# =====================================================================================================================

# 2ë²ˆ
1. Well-determined system: (X)
2. Over-determined system: (O)
3. Under-determined system: (X)
4. ì´ ì¤‘ì— ì •ë‹µì´ ì—†ë‹¤: (X)

# =====================================================================================================================

# 3ë²ˆ
1. Aê°€ Fat í–‰ë ¬ì´ë©´ ì´ ì‹ì— ëŒ€í•œ ë§ì€ ì†”ë£¨ì…˜ì´ ì¡´ì¬í•œë‹¤. (O)
2. ìœ„ ì‹ì˜ ë§ì€ í•´ ì¤‘ ì›ì ê³¼ì˜ ê±°ë¦¬ê°€ ìµœì†Œì¸ ì ì´ ìµœì í™” ë¬¸ì œì˜ ì •ë‹µì´ë‹¤. (O)
3. ìœ„ì™€ ê°™ì€ ìµœì í™” ë¬¸ì œëŠ” controlì— ì£¼ë¡œ ì‘ìš©ëœë‹¤. (O)
4. ìœ„ì˜ ì‹ì€ ì›ì ì„ ì§€ë‚˜ëŠ” ì§ì„ ì´ë‹¤. (X)

# =====================================================================================================================

# 4ë²ˆ

ìµœì í™” ë¬¸ì œì˜ ê¸°ë³¸ ìš”ì†Œ:

ã„± ëª©ì í•¨ìˆ˜ (Objective function):
ìµœì í™” ë¬¸ì œì˜ í•µì‹¬ìœ¼ë¡œ, ìµœëŒ“ê°’ì´ë‚˜ ìµœì†Ÿê°’ì„ ì°¾ê³ ì í•˜ëŠ” ëŒ€ìƒì„.
ì˜ˆ: ë¹„ìš©ì„ ìµœì†Œí™”í•˜ê±°ë‚˜ íš¨ìœ¨ì„ ìµœëŒ€í™”.

ã„´ ì˜¤ì°¨ (Error):
ìµœì í™” ë¬¸ì œì˜ í•„ìˆ˜ ìš”ì†ŒëŠ” ì•„ë‹˜.
ì˜¤ì°¨ëŠ” ë°ì´í„° ëª¨ë¸ë§ì´ë‚˜ íšŒê·€ ë¶„ì„ì—ì„œ ì”ì°¨(residual)ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë‚˜, ìµœì í™” ë¬¸ì œì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œë¡œëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ.

ã„· ì œì•½ì¡°ê±´ (Constraints):
ìµœì í™” ë¬¸ì œì—ì„œ í•„ìˆ˜ ìš”ì†Œ
ë³€ìˆ˜ë“¤ì´ ë”°ë¼ì•¼ í•  ì¡°ê±´(ì œì•½)ì„ ì •ì˜í•˜ë©°, ì´ë¥¼ ë§Œì¡±ì‹œí‚¤ë©´ì„œ ìµœì í™”ë¥¼ ìˆ˜í–‰í•¨.
ì˜ˆ: ğ´ğ‘¥â‰¤ğ‘, Axâ‰¤b, ğ‘¥â‰¥0

ë³€ìˆ˜ (Decision variable):
ìµœì í™” ë¬¸ì œì˜ ë˜ ë‹¤ë¥¸ í•„ìˆ˜ ìš”ì†Œ
ê²°ì • ë³€ìˆ˜ëŠ” ìµœì í™”ì˜ ê²°ê³¼ë¡œ ê²°ì •ë˜ëŠ” ê°’ì´ë©°, ëª©ì í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ê³  ì œì•½ì¡°ê±´ì„ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë¨

ã„±, ã„· ,ã„¹ 

# =====================================================================================================================

# 5ë²ˆ íŒ¨ìŠ¤

# =====================================================================================================================

# 6ë²ˆ
1. objective functionì´ non-convex functionì¸ ê²½ìš° local solutionì€ í•­ìƒ global solutionì´ ëœë‹¤. (X)
2. ì œì•½ì¡°ê±´ì´ ìˆëŠ” ê²½ìš°(constrained)ì™€ ì—†ëŠ” ê²½ìš°(unconstrained)ê°€ ìˆë‹¤. (O)
3. convex optimization ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ moduleë¡œ cvxpyê°€ ìˆë‹¤. (O)
4. modeling ë‹¨ê³„ì™€ solving ë‹¨ê³„ë¥¼ ë¶„ë¦¬í•˜ê¸° ìœ„í•˜ì—¬ ë“±ê°€ë³€í™˜ì„ í•œë‹¤. (O)

# =====================================================================================================================

# 7ë²ˆ íŒ¨ìŠ¤

# =====================================================================================================================

# 8ë²ˆ x â† xâˆ’Î±âˆ‡xf(x)ëŠ” Gradient Descent ì•Œê³ ë¦¬ì¦˜ì„ ë‚˜íƒ€ëƒ„. ì´ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì„ ê³ ë¥´ë¼ëŠ” ë¬¸ì œ

1. iterative methodì´ë‹¤.
Gradient DescentëŠ” ì´ˆê¸°ê°’ ğ‘¥0ë¥¼ ì„¤ì •í•œ í›„, ë°˜ë³µì ìœ¼ë¡œ ê°’ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ iterative methodê°€ ë§ìŒ.
â‡’ ì˜³ì€ ì„¤ëª…

2. ê¸°ìš¸ê¸°(âˆ‡ğ‘¥ğ‘“(ğ‘¥))ëŠ” ê¸°ì¡´ xì—ì„œ updateë  xë¡œ ì´ë™í•˜ëŠ” ë°©í–¥ì„±ì„ ê²°ì •í•œë‹¤.
ê¸°ìš¸ê¸°ëŠ” í˜„ì¬ ìœ„ì¹˜ì—ì„œì˜ ë³€í™”ìœ¨ì„ ë‚˜íƒ€ë‚´ë©°, ì´ ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ ì´ë™.
ì •í™•íˆëŠ” ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ë°©í–¥(-âˆ‡ğ‘¥ğ‘“(ğ‘¥))ìœ¼ë¡œ ì´ë™í•˜ì—¬ f(x0)ê°’ì„ ìµœì†Œí™”í•˜ë ¤ëŠ” ì•Œê³ ë¦¬ì¦˜ì„.
-> ì˜³ì€ ì„¤ëª…

3. alphaëŠ” atep sizeë¥¼ ê²°ì •í•œë‹¤.
alphaëŠ” í•™ìŠµë¥ (learning rate) ë˜ëŠ” step sizeë¡œ, í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ì—ì„œ ì´ë™í•˜ëŠ” í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤.
-> ì˜³ì€ ì„¤ëª…

4. Gradient Descent methodëŠ” ëª¨ë“  ì¡°ê±´ì—ì„œ ë¬´ì¡°ê±´ ìµœì í•´ë¡œ ìˆ˜ë ´í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
ì´ˆê¸°ê°’, í•¨ìˆ˜ì˜ í˜•í„”(ì˜ˆ: ë¹„ë³¼ë¡ í•¨ìˆ˜), í•™ìŠµë¥  alpha ë“±ì— ë”°ë¼ ìµœì í•´ì— ë„ë‹¬í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒ. í‹€ë¦¼.
-> í‹€ë¦¼

ìš”ì•½
Gradient DescentëŠ” ë§¤ìš° ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ì§€ë§Œ. í•­ìƒ ìµœì í•´ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì€ ì•„ë‹˜.
ì´ˆê¸°ê°’, í•¨ìˆ˜ì˜ ë³¼ë¡ì„±(convexity), í•™ìŠµë¥  ì„¤ì • ë“±ì— ë”°ë¼ ì§€ì—­ ìµœì í•´(local minimum)ì— ë¨¸ë¬¼ê±°ë‚˜ ìˆ˜ë ´í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ë„ ìˆìŒ -> ë”°ë¼ì„œ "ëª¨ë“  ì¡°ê±´ì—ì„œ ë¬´ì¡°ê±´ ìµœì í•´ë¡œ ìˆ˜ë ´í•œë‹¤"ëŠ” x

# =====================================================================================================================

# 9ë²ˆ. ë‹¤ìŒ ì¤‘ Convex functionì´ ì•„ë‹Œ ê²ƒì„ ê³ ë¥´ì‹œì˜¤.

Convex functionì€ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•¨.
2ì°¨ ë¯¸ë¶„ì´ í•­ìƒ 0 ì´ìƒ (ì¦‰, f''(x)â‰¥0)ì´ë©´ convexì„. ë˜ëŠ”, ê·¸ë˜í”„ê°€ ì•„ë˜ë¡œ ë³¼ë¡í•œ í˜•íƒœë¥¼ ê°€ì§€ë©´ convexë¡œ ê°„ì£¼

1. f(x) = x^2 
-> f''(x) = 2ë‹ˆê¹Œ  convexì„ (O)

2. f(x) = x^3
-> f''(x) = 6xì´ë‹ˆê¹Œ convexê°€ ì•„ë‹˜. xê°€ ìŒìˆ˜ì¼ ë•Œ 0ë³´ë‹¤ ë‚®ìœ¼ë‹ˆ (X)

3. |x|^0.5(=ë£¨íŠ¸)
-> convex (O)

4. f(x) = x^4 
-> f''(x) = 12x^2 ì´ë‹ˆê¹Œ convexì„ (O)

# =====================================================================================================================

# 10ë²ˆ. f(x1,x2) = (x_1)^2 + 2x_2^2 + 4x_1 + 3 ì— ëŒ€í•˜ì—¬ Learning rateë¥¼ 0.1ë¡œ ì„¤ì •í•˜ì˜€ë‹¤.
ì´ˆê¸°ì ì´ x1 = 1, 2 =1ì¼ë•Œ Gradient descent ì•Œê³ ë¦¬ì¦˜ ì„ í†µí•´ 1ë²ˆì˜ iterationì„ ì‹œí–‰í•œ í›„ì˜ x1ê³¼ x2ì˜ ê°’ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?

Gradient descent ì•Œê³ ë¦¬ì¦˜ 
xi <- xi - a(í¸ë¯¸ë¶„)

1ë‹¨ê³„) ê° ë³€ìˆ˜ì— ëŒ€í•œ í¸ë¯¸ë¶„ ê³„ì‚°
x_1 í¸ë¯¸ë¶„ : 2x_1 +4 -> 6
x_2 í¸ë¯¸ë¶„ : 4x_2 -> 4

2ë‹¨ê³„) ì•Œê³ ë¦¬ì¦˜ ì‹ ì ìš©
1) x_1
x_1 : x_1 - a(x_1í¸ë¯¸ë¶„)
= x_1 <- 1 - 0.1(6)= 1 - 0.6 = 0.4

2) x_2
= x_2 <- x_2 - a(x_2í¸ë¯¸ë¶„)
= x_2 <- 1 - 0.1(4) = 1 - 0.4 = 0.6