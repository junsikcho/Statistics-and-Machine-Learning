1번.
최소제곱해 문제.
틀림

=======================================================================================================

2번.
"위의 식을 보고 우리가 regression을 이용하여 찾고자 하는 변수가 무엇인지 고르시오."
yi = 세타0 + 세타1xi
xi = 독립 변수(입력값)
yi = 종속 변수(실제값)
​
회귀 분석(regression)은 독립 변수 x와 종속 변수y간의 관계를 나타내는
최적의 세타0와 세타1을 찾는 과정임.
정답 : 세타0과 세타1

=================================================================================================================

3번.
"Nonlinear regression에서는 x가 비선형적으로 분포하므로 linear regression을 이용해서 해결할 수 있다." (X)
이는 조건부로 참일 수도 있고 거짓일 수도 있다.
x가 단순한 변환으로 선형적으로 변환 가능하면 선형 회귀로 해결 가능.
하지만, 일반적으로 Nonlinear regression은 선형 회귀로 바로 해결할 수 없으므로, 이는 틀린 진술이다.
Nonlinear regression은 데이터가 비선형적 관계를 가질 때 사용하는 방법
Linear regression은 데이터가 선형적 관계를 가지거나, 선형 변환이 가능할 때 사용됨

=============================================================================================================

4번.
RBF function을 정의하기 위해 필요한 것으로 옳지 않은 것을 고르시오.

1) RBF basis의 개수
- RBF를 사용할 때, 몇개의 BASIS를 사용할지 정해야함. (필요)

2) 각각의 RBF basis의 center
- 각 RBF basis는 특정 center을 기준으로 거리를 계산.
중심이 정의되지 않으면 RBF를 제대로 사용할 수 없음. (필요)

3) RBF basis 간의 유사도와 거리차
- 유사도와 거리차는 RBF를 정의하는 것이 아니라, 계산 후에 나오는 값임.
결과에 영향을 줄 수도 있지만, 필요한 요소는 아님. (필요X)

4) 표준편차(폭)은 Gaussian RBF의 형태를 결정하는 중요한 요소임.(필요)

=============================================================================================================

5번.
과적합(overfitting)을 해결하기 위한 방법으로 옳지 않은 것을 고르시오.

1) fitting function의 차수를 낮춘다.
- 과적합은 데이터에 지나치게 잘 맞는 경우 발생. 차수를 낮추면 모델의 복잡도가
줄어들어 과적합을 줄이는 데 효과적. (해결 방법O)

2) fitting function의 차수를 높인다.
- 차수를 높이면 오히려 모델의 복잡도가 증가하여 데이터에 더 잘 맞추려고 함.
오히려 과적합을 심화시킬 가능성이 높음 (해결 방법x)

3) regularization을 추가해 준다.
- Regularization(정규화)은 모델의 복잡도를 제한하거나 과도한 가중치를 억제하여
과적합을 줄이는데 도움을 줌. 대표적으로 L1, L2 정규화가 있음 (해결 방법O)

4) 다른 종류의 fitting function을 사용해 본다.
- 데이터에 더 적합한 모델을 선택하면 과적합 문제를 완화할 수 있음. (해결방법 O)

============================================================================================================

6번.
"비선형 회귀 문제를 해결하는 방법으로 옳은 것을 모두 나열한 답을 고르시오."

ㄱ) Polynomial features를 이용
- 이 방법은 데이터를 다항식 형태로 변환하여 비선형 관계를 모델링하는 방법.
형태를 확장하면 비선형 회귀문제를 해결할 수 있음 (O)

ㄴ) y = c로 회귀(c는 상수)
-  y=c는 상수를 예측하는 방식으로, 어떠한 비선형 패턴도 학습할 수 없음.
(해결 X)

ㄷ) RBF features를 이용
- RBF는 데이터를 비선형적으로 변환하여 고차원 공간에서 선형 관계로 변활할 수 있도록 도움.
(해결 O)

ㄹ) Kernel trick 이용
- Kernel trick은 데이터를 고차원 공간으로 매핑하는 효과를 내는 방법.
비선형 데이터를 처리할 수 있음.
(해결 O)
===============================================================================================================

7번. 
"denoising된 signal의 smoothness를 높이려면 
μ를 감소시켜야 한다"는 명제가 맞는지 판단하는 문제

-> μ는 smoothness를 조절하는 가중치로, 값이 클수록 신호 간 급격한 변화가
억제되어 더 부드러운 신호를 얻을 수 있음.
따라서, smoothness를 높이려면 μ를 감소시키는 것이 아니라, 증가시켜야함.

===========================================================================================

8번.
"과적합(overfitting)은 데이터 수가 적은 경우에 발생할 가능성이 높으므로, 빅데이터를 사용하면 과적합이 일어나지 않는다."
O, X?

-과적합의 원인 
-> 모델이 학습 데이터에 너무 잘 맞아서 일반화(generalization)를 잘하지 못하는 문제.
1) 데이터가 적을 때 -> 데이터 부족 시, 패턴이 아니라 노이즈를 학습하기 쉬워짐
2) 모델이 너무 복잡할 때 -> 모델의 복잡도가 데이터 복잡도보다 높으면 과적합 발생 가능성이 높음
3) 복잡한 모델에선 빅데이터를 사용해도 과적합 발생할 수 있음. 그러므로 정규화(regularization)같은 기법 필요

즉, 빅데이터는 과적합 가능성을 줄이지만, 완전히 방지하지는 못함.

===============================================================================================

